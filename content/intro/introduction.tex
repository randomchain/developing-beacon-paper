\section{Introduction}
A \emph{randomness beacon}, i.e.\ a service emitting unpredictable random values at fixed intervals, is not a new concept.
In 1983 Michael O.\ Rabin coined the term and utilized one to add probabilistic security in several protocols~\cite{rabin1983transaction}.
In this definition, a randomness beacon was to be seen as an impartial third-party trusted to be unbiased towards any outcome.
As such, you should trust the beacon operator (the entity running the beacon service) to not be biased since you can not verify that they were unbiased.

For quite a while, randomness beacons did not receive much attention, likely because alternatives to Rabin's protocols not requiring a trusted beacon were used instead (such as~\cite{BGMR}).
In circa 2010, a renewed interest in beacons was seen as an increase in beacon-related literature; the trend in this new literature was to remedy the need to trust the beacon operator.
We believe it was a reaction to revelations like the \gls{nsa} whistle-blower leaks that diminished people's trust in authorities.
In other words, people had their eyes opened to the fact that \emph{trust} can be an issue in itself and that removing the need for trust in any one entity could be beneficial.
As an example, cryptocurrencies have flourished in recent years alongside a sharp rise in the popularity of blockchains --- two technologies seeking to facilitate cooperation of mutually distrustful users.

Conceptually, randomness beacons seem to fit this environment of minimizing the need to trust.
However, the \enquote{old} way, requiring users to trust the operator, simply shifts the trust issue to the centralized entity, i.e.\ the beacon operator.
In the \enquote{new} beacons described in the recent literature, the beacon is acting as an impartial party.

The merit of a randomness beacon lies in contexts where a set of users needs to agree on some random outcome, but do not trust each other or an impartial third party to make the decision.
Therefore, a randomness beacon is \emph{not} required in the case a user needs randomness for just themself. In this case, standard ways of generating randomness on a computer are far easier.
Similarly, if users trust each other or a third party, the randomness generation is also trivial.
A randomness beacon is not necessarily generating \enquote{more random} numbers --- it merely allows users to agree on the same randomness without trusting anyone.

Even though the literature theoretically argues for a variety of solution, we have not seen many implementations of randomness beacons.
We believe that bridging a theoretical design and practical implementation is uncharted territory.
Moreover, the literature which somewhat explore this bridge either culminate in a highly specialized solution often unfit for a public context~\cite{syta2017scalable,cascudo2017scrape}, or disregards a thorough security analysis and trust assessment \mtjnote{Cite examples}.
Throughout this paper we design, implement, and evaluate a randomness beacon, which borrows from existing research while introducing novel ideas; both in the system but also the operation of it.

In any case, randomness beacons are interesting as a concept, and we feel this concept needs further exploration in order to be properly utilized in real world applications. This paper will be a step in that direction.

\subsection{Terminology}
In this work we use the terms \enquote{randomness beacon} and \enquote{beacon} interchangeably.
As presented, we see two distinct groups of randomness beacons: the ones requiring users to blindly trust the beacon operator (Ã  la Rabin's original beacon), which we refer to as \emph{trust-requiring} beacons; and the \enquote{new generation} randomness beacons of recent literature, which convincingly prove to all users of the beacon that nothing dubious happened during the creation of the random output --- these are referred to as \emph{trust-minimizing} beacons.

\subsection{Security Goals}\label{sec:security_goals}
A trust-requiring beacon simply shifts the issue of trust.
Therefore, we strive to design and implement a trust-minimizing beacon that works on the most pessimistic assumption possible:
\enquote{%
Everybody, including any beacon operator if present, is secretly colluding against you and is willing to put an unlimited amount of money and resources towards manipulating or biasing the randomness.
As such, you can only trust yourself.}

These assumptions describe the mindset we take on while designing and implementing the beacon.
As we also need to account for the feasibility of the system in real world applications, we will consider the above assumption as non-binary.
A completely trustless system may not be practically feasible today, and instead we seek to minimize the trust required and ultimately let each user decide how much they \emph{want} to trust.
In essence, a user will know that under self-chosen trust assumptions, the randomness has not been manipulated.

\subsection{Beacon Context}
As stated, beacons are relevant in contexts where several users want to agree on some random outcome, but do not trust anyone to solely decide that outcome --- a pattern which fits a number of use cases.

Consider the generic use case of sampling.
Essentially, sampling is about selecting representative data points, potentially with high stake consequences.
It would not be far-fetched to imagine someone wanting to bias this sampling process in order to skew the results.
One such sampling process is lotteries, which need to randomly sample a pool of participants to draw winners.

The field of cryptography also contains use cases.
Many cryptographic schemes require some constants in the design of algorithms, and it has been shown that some schemes have been intentionally built with specific constants in order to facilitate a backdoor~\cite{nist2014backdoor}.
Selecting constants with a randomness beacon can prove to the users of such cryptographic schemes that the constants were not manipulated and thus are unlikely to contain backdoors~\cite{baigneres2015trap}.
One could even in some use cases imagine a \enquote{refreshing} algorithm where constants are variables which change with new beacon outputs.

Staying in the field of cryptography, \glspl{zksnark} require a lengthy process of initial bootstrapping.
In systems such as \textit{zcash}\footnote{\url{https://z.cash}}, this bootstrapping has been performed by a \gls{mpc}~\cite{snarkparameters}.
However, the \gls{mpc} scales poorly because of the many rounds of communication needed between parties alongside big amounts of data to ensure a fair output.
\citet{mpcsnarks} suggest avoiding the heavy communication and computation of the \gls{mpc} and instead having users directly contribute a random number.
To avoid the last user carefully choosing their input to manipulate the bootstrapping to their benefit (a so-called \emph{last-draw attack}), an output from a randomness beacon is applied as the last input.
Thus a lengthy \gls{mpc} is substituted with a quick round of user input and sealing the deal with a randomness beacon.
\mtjnote{Sass, can we find any sources saying that tedious bootstrapping is a hindrance for the adoption of zksnarks?}

\subsection{Beacon Concepts}

A randomness beacon emits an unpredictable random value at a regular interval, e.g.\ every five minutes.
\cref{fig:abstract_beacon} shows the workings of a simple, generic beacon.
The beacon performs \emph{some} computation on an input source in order to generate an unpredictable number.
The result of the computation, the random number, is the output, and is sent to users.
This workflow is repeated indefinitely at the specified interval.
\subimport{}{simple_beacon_fig.tex}

Examining existing beacons, a few common ways of composing the input sourcing and computation are discernible and can be described as specific models.
Here, we distinguish between an input source model and operational model.
The input source model describes the way the beacon sources its input, while the operational model describes the design of the protocol, i.e.\ how to perform the computation and publish the output.
These models are based on our earlier work \fxnote{Insert reference to own work previous semester?}.

\subsubsection{Input Source Models}
\mtjnote{Cite ourselves} describe three sources of input:

\paragraph{Private Input Sources}
A beacon can use some private source of data to produce randomness.
This allows them to produce randomness of high quality at a high rate, but requires users of the beacon to trust the beacon and its randomness.
A single entity producing their own randomness as input cannot be trusted in our setting, since it cannot reliably be distinguished from carefully crafted values that appear to be random.
An example of this is the \gls{nist} randomness beacon~\cite{nistbeacon} that observes quantum mechanical effects to produce what they claim is high-quality randomness.
Ultimately, it requires trust, since the observations cannot be repeated, and therefore users cannot make sure that the value is indeed from observing the quantum mechanical effects.
As such, the users need to blindly trust the beacon operator, which in the case of \gls{nist} can be hard given their history~\cite{nytimes-nsabackdoors, nytimes-nsaconstants, nist2014backdoor}.

\paragraph{Publicly Available Sources}
Using a publicly available source that everyone can agree on the value of, such as bitcoin blocks, stock market data, or lottery winning numbers from several international lotteries.
The user must trust the source, and this is reasonable since these sources are governed by some guarantees.
In case of bitcoin, the blocks have a monetary value and are virtually impossible to forge.
Users have to interact with the source to indirectly influence the beacon and prevent biased outputs.
However, it may also be harder for adversaries to bias the beacon through the source unless they are in complete control of the given source.

\paragraph{User Input}
A user can be allowed to directly provide input to the beacon.
The idea is that a user provides a value that they firmly believe is sufficiently random, such that nobody could have predetermined the given value.
There exists no concept of ownership of specific values, which means that users should trust the input they give, and not the fact that it is their own.
The beacon then performs an operation on a set of user-supplied inputs.
The output of the beacon is structured in a way that
\begin{eletterate*}
    \item allows all users to verify the inclusion of their value and
    \item allows all users to verify the validity of the computation.
\end{eletterate*}

If these are satisfied, the user knows that a value they trust to be random has been part of the random output generation.
The computation performed by the beacon should ensure that users cannot knowingly bias the output to anyone's disadvantage.
As such, the user knows that his input was not knowingly \enquote{counteracted} by another used.

\subsubsection{Operational Models}
\mtjnote{cite ourselves} identify three ways in which a beacon is typically operated:

\paragraph{Autocratic Collector}
A beacon is run by a party, which deems it irrelevant to prove honesty, thus requiring blind trust from the users.
As such, the computation is a black box with no possibility for proof of honesty.
This type lies in the trust-requiring category of beacons, i.e.\ the old generation.

\paragraph{Specialized \acrshort{mpc}}
Users utilize \acrfull{mpc} to collectively produce randomness, typically from their own inputs.
Given an honest majority, this type of beacon produces randomness that is not biased against the participants, and although work has been done in the field, they are difficult to scale to large groups~\cite{cascudo2017scrape, syta2017scalable}.
This type of beacon is therefore badly suited for public settings, but might fit in a controlled private context.

\paragraph{Transparent Authority}
A single entity collects input and publishes it with a focus on transparency.
Users can, by observing the beacon, verify that it behaves according to protocol.
This does not directly prevent byzantine behavior, but rather makes it difficult or nearly impossible to hide such behavior.
This type also support a wide variety of implementations, and is potentially scalable to a public setting.

\subsection{Delimitations}%
\label{sub:delimitations}
We want to create a randomness beacon that is secure under the assumption that everyone may be colluding against a given user, as per our security goals in \cref{sec:security_goals}.
As such, we can immediately see that the autocratic collector is not suitable for our security goal, because it requires users to trust its claimed honest operation.
The \gls{mpc} model does not scale well enough for general use in public randomness beacons~\cite{syta2017scalable}.
As described by \citet{damgaard2006scalable}, most \gls{mpc} protocols either assumes an honest majority, which is a weaker assumption than our security goals, or cannot guarantee fairness and output delivery, a likewise undesired behavior.
Even as more scalable \gls{mpc} protocols has been developed~\cite{damgaard2006scalable}, they still need some assumption about the protocol participants, which we cannot guarantee in a public randomness beacon.
Therefore, \gls{mpc} is not be suitable for us, although the model could fit in a more controlled or private environment.
This leaves us with the transparent authority model, which we adopt.

Regarding input source models, we can immediately discard private input sources as they are tied to the autocratic collector model, and as such do not work for the transparent authority.
The guarantees by publicly available sources are weak compared to user input.
If sufficiently paranoid, the user will want to bias these publicly available sources to make sure all other users are not colluding.
Therefore, user input is the simplest solution and provides the strongest guarantees for the user.

\subsection{Contributions}
We bridge the gap \stefan{you have not really established this gap yet: is there not quite a bit of theory on random beacons already? can you quickly review it and then say why it is "too theoretical"?} between theoretical solutions and the real world by designing and implementing a secure, trust-minimizing randomness beacon based on the transparent authority model with user input.
The design is based on a structured analysis of threats to a randomness beacon.
We design the beacon from the ground up based on tried and tested methods as well as novel ideas.
Specifically, we differ from previous transparent authority approaches by the following:
\begin{itemize}
    \item We describe a novel way of parallelizing the beacon pipeline to provide a higher trust probability.
    \item Unlike all other approaches we have seen, the beacon operator in our beacon design has no private information --- all inputs are hashed and are released to the public in batches.
    \item We choose to use Merkle trees as the data structure for inputs.
    \item We allow multiple input channels and output channels to be instantiated for reliability, distribution of workload, easy scaling, and convenience for the users.
\end{itemize}
Combined, our novel ideas significantly decreases the need for blind trust and limits the severity of a myriad of attacks.
Lastly, we explore usage of our beacon in context. As such, we describe several use cases and how the beacon output is used as a cryptographic primitive in a way that aligns with and extends our security goals.
\stefan{What about performance analysis?}